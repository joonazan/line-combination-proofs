\documentclass[english, 12pt, a4paper, sci, a-1b, online]{aaltothesis}

\usepackage[numbers]{natbib}
\usepackage{graphicx}
\usepackage[most]{tcolorbox}
\usepackage{amsmath, amsthm}
\usepackage{unicode-math}
\setmathfont{texgyrepagella-math.otf}

\usepackage{tkz-graph}
\usepackage{tikz}
\usetikzlibrary{calc}
\usetikzlibrary{arrows.meta}

\usepackage{minted}
\usemintedstyle{friendly}
\setmonofont{DejaVu Sans Mono}
\setminted{fontsize=\footnotesize, autogobble}
\newcommand\icoq[1]{\mintinline{Coq}{#1}}
% horrible hack to hide pygment's complaints about unicode
\AtBeginEnvironment{minted}{\dontdofcolorbox}
\def\dontdofcolorbox{\renewcommand\fcolorbox[4][]{##4}}

\usepackage{graphicx}
\graphicspath{{./images/}}

\degreeprogram{Computer, Communication and Information Sciences}
\major{Computer Science}
\code{SCI3042}

\univdegree{MSc}
\thesisauthor{Joonatan Saarhelo}
\thesistitle{Fast and Correct Round Elimination}
\place{Espoo}
\date{2022}

\supervisor{Prof.\ Jukka Suomela}
%\advisor{}

%% \uselogo{aaltoRed|aaltoBlue|aaltoYellow|aaltoGray|aaltoGrayScale}{?|!|''}
\uselogo{aaltoRed}{''}

\keywords{For keywords choose\spc{}concepts that are\spc{}central to your\spc{}thesis}

\thesisabstract{
Round Elimination is a process used to study the hardness of distributed graph problems.
It is implemented as a computer program and optimizations to the program have permitted the study of more complex problems.
I invented a new algorithm for maximization, the slowest part of Round Elimination, proved that the new algorithm is asymptotically faster and formally verified the theory behind it in Coq.
I wrote a program that checks maximization results using the new algorithm and verified it as well.
Unfortunately the verified program is too slow for practical use but a nearly identical program TODO
}

%% Copyright text. Copyright of a work is with the creator/author of the work
%% regardless of whether the copyright mark is explicitly in the work or not.
%% You may, if you wish, publish your work under a Creative Commons license (see
%% creaticecommons.org), in which case the license text must be visible in the
%% work. Write here the copyright text you want. It is written into the metadata
%% of the pdf file as well.

\copyrighttext{Copyright \noexpand\copyright\ \number\year\ \ThesisAuthor}
{Copyright \textcopyright{} \number\year{} \ThesisAuthor}

\begin{document}

\makecoverpage{}

\makecopyrightpage{}

\begin{abstractpage}[english]
  \abstracttext{}
\end{abstractpage}

\newpage

\thesistitle{Nopea ja virheetön kierroseliminaatio}
\keywords{Vastus, resistanssi, lämpötila}

\begin{abstractpage}[finnish]
  Tiivistelmässä on lyhyt selvitys
  kirjoituksen tärkeimmästä sisällöstä: mitä ja miten on tutkittu,
  sekä mitä tuloksia on saatu. 
\end{abstractpage}

\thesistableofcontents{}


\mysection{Symbols and abbreviations}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}{Lemma}[section]
\newtheorem{corollary}{Corollary}[theorem]

\newcommand{\reline}[1]{\textbf{#1}}

\tikzstyle{active} = [draw, thick, circle, fill=white, minimum size=1.5em]
\tikzstyle{passive} = [draw, thick, circle, fill=darkgray, text=white, minimum size=1.5em]

\subsection*{Symbols}

\begin{tabular}{ll}
  $\Delta$ & graph maximum degree \\
  $\Pi_n$    & round elimination problem \\
  $\Sigma_n$ & alphabet of $\Pi_n$ \\
  $A_n$      & active side configurations of $\Pi_n$ \\
  $P_n$      & passive side configurations of $\Pi_n$ \\
\end{tabular}

\subsection*{Abbreviations}

\begin{tabular}{ll}
  RE         & round elimination \\
  GADT       & generalized algebraic data type
\end{tabular}

\cleardoublepage{}
\section{Introduction}
%% Leave page number of the first page empty
\thispagestyle{empty}

Today's world is filled with software and most of it does not always work as intended. The consequences range from mere annoyances to expensive or life-threatening incidents.

Programming languages that completely eliminate invalid memory accesses or even the bulk of crashes have entered the mainstream. Yet all mainstream languages are Turing-complete and as a consequence allow writing programs that hang indefinitely. (C compilers are allowed to assume that loops with no side effects terminate~\cite{C11}, so arguably C is not Turing-complete and recognizing valid C programs is undecidable.) Writing programs that not only run to completition but even behave as intended is still commonly viewed as infeasible or prohibitively expensive.

At the same time, computers have become an integral part of mathematical research. This is rather surprising – while mathematicians have published many mistakes~\cite{itpsurvey}, most programs contain several. Still, computer programs are good for testing conjectures on millions of different inputs in the hope of finding a small counterexample that is easy to check by hand. But not all outputs are easy to check.

Round elimination is a process used in proofs about the time complexity of distributed algorithms. Even the best known method of checking the result of RE implements most of RE, so the only way to produce reliable results is to have a reliable implementation.

Round elimination can be tedious or downright impossible to perform by hand, as it may have to be applied multiple times in succession with every application multiplying the size of the working set. Faster algorithms have allowed RE to be used on a wider variety of problems, so speeding it up is desirable. 

Hence, the goal of this thesis is to produce a faster but also trustworthy implementation of round elimination. I tackle only one part of RE, maximization, as that part accounts for most of the computation time and is easy to specify.

Section~\ref{resection} explains how to perform round elimination and how it helps in analyzing the class of problems introduced in Section~\ref{lclsection}. Section~\ref{nlproof} is a proof sketch derived from the mechanized proof discussed in Section~\ref{mechproof}, which I wrote for the new maximization algorithm that I developed. Finally, Section~\ref{performance} compares the asymptotic performance of the new algorithm to the state of the art.

\clearpage
\section{Background}

\subsection{Methods of improving correctness}

Unlike the famous Djikstra quote suggests, testing can be used to show the absence of bugs but only in programs that take a small amount of different inputs. When feasible, this is a very good method. The only downside is limited composability: using the result of one test to optimize another quickly gets messy.

Fuzzing means running software against computer-generated inputs to find crashes. AFL-fuzz~\cite{AFL} is a highly optimized fuzzer that monitors program execution in order to find inputs that trigger previously unexplored execution paths. It has been used to find intricate bugs in complex software. One crash that I have found via fuzzing was caused by code that assumed that one can compute the absolute value of any machine integer. However, taking the absolute value of the smallest representable integer (for example -128 for 8-bit integers) leads to undefined behaviour because there is always one negative integer more than there are positive integers in two's complement.

Because of the program instrumentation, fuzzing is able to find interesting inputs even though it cannot go through the whole input space. It optimizes code coverage of the executable rather than the source code. Fuzzing is best at finding crashes but it can be used to find incorrect behaviour by writing a program that crashes whenever an incorrect output is produced. However, that is only effective if outputs can be checked very quickly. Fuzzing is of little use in the case of round elimination because RE it is very slow to perform and there is no way to check the result except by running round elimination on it. At best, one could compare a new algorithm to a simpler but even slower implementation of RE.\@

SAT solvers are programs that are optimized for solving the boolean satisfiability problem. They can be used to find cases where formulas fail on finite but relatively large inputs and even perform an exhaustive search to show that no such cases exist. The biggest downside is that turning algoriths into boolean formulas can be challenging.

Just like fuzzers, SAT solvers suffer from the problem that round elimination is difficult to check. If it was easy to check, one could just ask a SAT solver to find a solution. I actually did that with Z3 but as expected, the performance was bad because the input was pretty big.

Formal proof can establish correctness of algorithms even for arbitrarily large inputs and is completely unaffected by how expensive it is to compute or check outputs, which makes it ideal for verifying round elimination. Unfortunately, theories that are simple to state can have very large proofs.

\subsection{History of computer-aided mathematics}

Perharps the most notable example is Thomas Hales' proof of the Kepler conjecture. The Kepler conjecture states that there is no way of packing spheres more densely than the cannoball packing, in which the spheres are arranged in hexagonally packed layers and those layers are stacked on top of each other. In 1831, Gauss proved a proposition about polynomials that can be used to show that a variant of the aforementioned packing is the densest possible lattice packing of spheres.~\cite{dichteste} However, before Hales' proof the possibility of a denser irregular packing wasn't ruled out.

A proof consisting of computer programs that (among other things) solved linear programming problems and hundreds of pages of notes was completed in 1998. After four years of inspecting the proof, a panel of twelve referees announced that they couldn't be completely certain if the computer calculations were correct. That prompted Hales to start the Flyspeck project, which completed a formal proof of the Kepler conjecture in 2014 and found a few bugs in the original code.~\cite{itpsurvey}

TODO

\subsection{Proof automation}

Completely formal proofs can be large and repetitive and a minute change in the theorem to be proved can change their shape. Directly constructing proofs by hand seems like a fool's errand. Instead, the bulk of proofs is produced via various forms of code generation.

Most proof assistants read a proof script from disk and generate the proofs anew on every run. The only exception that I'm aware of is Metamath~\cite{MMZero}, in which a simple program checks a proof file that is not human-readable. The proof file is typically produced with untrusted tools, as only the checker needs to be correct.

The other tools prefer human-readable proofs because they are maintainable. You may wonder why a proof needs to be maintainable, as once it is complete, it doesn't need to be modified anymore. The reasons are the same as in software engineering in general: while the proof is in development, there may be unanticipated changes.

As an example, it may turn out that there is a data structure that is easier to work with than the one initially used. \emph{Transporting} the old proofs to the new data structure takes great effort. On the other hand, automated proofs will often work with a different data structure with little to no editing. Besides, if the initial formalization of the mathematical object in question was inaccurate, there is no other option than to change data structures.

\subsubsection{Aside: transporting proofs}\label{transport}

It is very common in mathematics to prove something for one representation of a mathematical object and assume that it is true for all isomorphic representations. This notion, while intuitive is not trivial to formalize and current proof assistants do not support it by default.

An easy way to make a proof that applies to all representations of an object is to write a proof that applies to anything with certain properties. For example, instead of relying on a particular representation of natural numbers, one can write a proof that applies to anything satisfying the Peano axioms. But that makes proofs more difficult as they cannot reap the benefits of any particular representation.~\cite{transferAlongIso}

An important motivation for transporting proofs between representations is \emph{refinement}, replacement of data structures with more efficient ones.~\cite{refinementsFF} In fact, the development discussed in this work would be far more useful were the data structures refined to to ones that are of practical use. This is elaborated on in Section~\ref{badds}.

Unlike other attempts, Univalent foundations~\cite{hottbook} support transporting any function over equivalences. However, it is fundamentally different from the Calculus of Inductive Constructions, the underlying logic of Coq, which is the Coq implementation involves a Univalence axiom. As axioms in Coq have no computational meaning, transported functions cannot be executed, which is not a problem for proof but renders transported executable code useless. There are newer type systems, for instance Cubical type theory~\cite{cubicalTT}, which assign computational meaning to univalence. Equivalences for Free~\cite{equivalencesFF} describes an approach that manages to transport most useful constructs while remaining in Coq.

\subsubsection{Tactics languages}

TODO even Automath had an automation language

\subsubsection{Type classes}

Haskell type classes and Rust traits are an example of code generation in mainstream programming languages. These features select an appropriate function depending on the arguments' types. For example, the Haskell type class Show is implemented for integers and for lists of things that implement Show. These implementations are automatically composed when printing a list of integers.

Canonical structures fullfill a similar purpose to type classes in Coq. A notable difference is that canonical instances are selected via unification. Gonthier et al.~\cite{overloadingCanonical} exploit the fact that Coq only unfolds terms when unification fails to automatically try multiple instances in sequence. By writing instances akin to a logic program, one can automate things like ordering terms correctly in order to apply a lemma.

I don't use this kind of automation in my code, as I only learned about it afterward, but it seems highly useful. Reordering things is easy but code that has to explicitly state the desired order of terms before applying a lemma is tedious and breaks when the reordered expression changes.

\clearpage % otherwise the figures in the next section don't behave

\section{Locally Checkable Labeling}\label{lclsection}

Locally checkable labeling (LCL) is a family of graph problems in which one has to assign colors to edges or vertices, satisfying some locally checkable condition.~\cite{LCL}

Locally checkable means that a solution can be checked by checking fixed size neighborhoods. For example, to verify that vertices are properly colored, one just has to look at the neighbors of each vertex and check that no neighbor has the same color.

When every vertex has at most $\Delta$ outgoing edges, an LCL-problem can be given as a finite set of allowed neighborhoods, where $\Delta$ is the maximum degree of the graph.

\newcommand\tick{\fill[scale=0.6, color=black!40!green](0,.35) -- (.25,0) -- (1,.7) -- (.25,.15) -- cycle;}
\newcommand\cross{\draw[scale=0.3, very thick, color=red] (0,0) -- (1, 1) {} (0, 1) -- (1, 0) {};}

\begin{figure}[h]
\centering
\begin{tikzpicture}[line width=1pt]
  \foreach \i/\a/\b/\c/\ok in {0/-latex/latex-/latex-/\tick, 1/-latex/-latex/latex-/\tick, 2/-latex/-latex/-latex/\tick, 3/latex-/latex-/latex-/\cross}
  {
    \tikzset{xshift={\i * 7em}}
    \node [active] at (0,0) (center) {};
    \draw [\a] (center) -- (-90:1);
    \draw [\b] (center) -- (30:1);
    \draw [\c] (center) -- (150:1);
    \tikzset{xshift=1.4em, yshift=-1.8em}
    \ok
  }

  \tikzset{yshift=5em}
  \foreach \i/\a/\b/\ok in {0/-latex/latex-/\tick, 1/-latex/-latex/\tick, 3/latex-/latex-/\cross}
  {
    \tikzset{xshift={\i * 7em}}
    \node [active] at (0,0) (center) {};
    \draw [\a] (center) -- (-1,0);
    \draw [\b] (center) -- (1,0);
    \tikzset{xshift=1.4em, yshift=-1.8em}
    \ok
  }

  \tikzset{yshift=5em}
  \foreach \i/\a/\ok in {0/-latex/\tick, 3/latex-/\cross}
  {
    \tikzset{xshift={\i * 7em}}
    \node [active] at (0,0) (center) {};
    \draw [\a] (center) -- (-1,0);
    \tikzset{xshift=1.4em, yshift=-1.8em}
    \ok
  }
\end{tikzpicture}
\caption{An exhaustive list of neighborhoods for checking sinkless orientation in a graph with maximum degree three.}
\end{figure}

I will use \emph{sinkless orientation} as a running example. It is the problem of selecting a direction for each edge so that no vertex has only incoming edges. To check sinkless orientation, it is suffices to see the neighboring edges. There are other problems that require seeing a larger neighborhood, such as distance-2 coloring, the problem of choosing different colors for every pair of vertices the shortest distance of which is two.

\newcommand\samplegraph
{
  \node [active] at (0,0) (a) {};
  \node [active] at (1,0) (b) {};
  \node [active] at (2,0) (c) {};
  \node [active] at (1.5,1) (d) {};
}

\begin{figure}[h]
\centering
\begin{tikzpicture}[line width=1pt]
  \samplegraph
  \draw (a) -- (b) -- (c) -- (d) -- (b);

  \tikzset{xshift=10em}
  \samplegraph
  \draw [-latex] (a) -- (b);
  \draw [-latex] (b) -- (c);
  \draw [-latex] (c) -- (d);
  \draw [-latex] (d) -- (b);
\end{tikzpicture}
\caption{An undirected and the unique solution to sinkless orientation in it.}
\end{figure}

\subsection{Round Elimination problems}

\emph{Biregular trees} are trees that have properly two-colored vertices and where the degree of each class of vertices is constant. Round elimination operates on edge coloring LCL-problems in biregular trees that can be checked by only checking the nearest edges' colors. I call this kind of problem a \emph{round elimination problem} or RE problem.

Sinkless orientation is not such a problem. To begin with, edge orientation is not a color. However, there is an isomorphism between the solutions our sinkless orientation problem and the solutions of an RE problem, as we will see in a moment.

One of the classes of the biregular tree is called the \emph{active side} and the other the \emph{passive side}. The active side nodes are active in the sense that they are the computers in a network that have to choose the colors of their neighboring edges.

Since each neighborhood consists of only a vertex and a number of edges, we can simply list all multisets of allowed edge colors. They are called \emph{configurations} and are denoted with square brackets in this thesis. There are two sets of configurations, the configurations around active nodes, $A$, and the ones around passive nodes, $P$.

An RE problem can be represented as sets $A$ and $P$. The symbols in the configurations are from an alphabet $\Sigma$.

\subsection{Conversion from LCL}

I will now show how to convert sinkless orientation to an RE problem. The RE problem operates on a tree where the degree of active nodes corresponds to the original graph's degree and the degree of passive nodes is two. Each vertex in the original graph corresponds to an active vertex in the RE graph. Edges correspond to passive vertices. We can describe the directions of edges by coloring the RE graph's edges with I (for incoming) and O (for outgoing).

\begin{figure}[h]
\centering
\begin{tikzpicture}[line width=1pt]
  \node [active] at (5, 0) (center') {};
  \node [active] at (0, 0) (center) {};
  \foreach \a/\cl/\el/\arrow in {0/O/I/-latex, 120/O/I/-latex, 240/I/O/latex-}
  {
    \node [active] at (\a+18:1.4) (A){};
    \draw [\arrow] (center) -- (A)
    coordinate[near start] (tail)
    coordinate[near end] (head);

    \ifnum\a=240
      \node [passive, label=right:{passive}] at ($ (\a+18:1.5) + (center') $) (P) {};
      \node [active, label=right:{active}] at ($ (\a+18:3) + (center') $) (A') {};
    \fi
    \node [passive] at ($ (\a+18:1.5) + (center') $) (P) {};
    \node [active] at ($ (\a+18:3) + (center') $) (A') {};

    \draw (center') -- (P) node[midway, sloped, below] (CtoP) {\cl};
    \draw (P) -- (A') node[midway, sloped, below] (PtoA) {\el};
    \draw [dotted] \foreach \da in {-60, 60} {
      (A) -- ($ (A) + (\a+18+\da:0.8) $)
      (A') -- ($ (A') + (\a+18+\da:1) $)
    };
  };
  \draw [dashed, stealth-stealth, shorten <=0.6ex, shorten >=0.5ex] (tail) to[bend left=10] (CtoP);
  \draw [dashed, stealth-stealth, shorten <=0.5ex, shorten >=0.5ex] (head) to[bend left=10] (PtoA);
\end{tikzpicture}
\caption{Correspondence between solutions in round elimination graph and original graph}
\end{figure}

The active side's rule $A$ lists all the valid neighborhoods. The passive side's rule $P$ ensures that each edge is incoming on one end and outgoing on the other; otherwise there could be edges with an arrowhead on both sides.
\begin{align*}
\Sigma &= \{I, O\} \\
A &= \{[I, I, O], [I, O, O], [O, O, O]\} \\
P &= \{[I, O]\}
\end{align*}

The cases of sinkless orientation that deal with nodes of lower degree can be disregarded, since the active nodes in the biregular tree all have the same degree. The RE problem only models sinkless orientation on regular trees.

Real-world graphs are not regular trees, since graphs without any vertices of degree one are infinite. But proofs about regular trees are still useful: if something is impossible on a regular tree, it is impossible on graphs. Some interesting graphs are almost regular trees, one just has to handle the edge cases.

The same kind of conversion as done here to sinkless orientation be applied to any locally checkable problem on regular trees. It isn't always as straightforward, especially if the LCL problem looks at a big neighborhood but it is possible.

For example, distance-2 vertex coloring is quite verbose. In it, neighboring vertices have to have different colors and vertices at distance two have to have different colors. It can be encoded as follows for degree two graphs.
\begin{align*}
A &= \{[(c_0, c_1, c_2), (c_0, c_2, c_1)] \mid \text{for all distinct } c_0, c_1\text{ and }c_2 \} \\
P &= \{[(c_1, c_2, c_3), (c_2, c_1, c_4)] \mid \text{for all distinct } c_1, c_2, c_3\text{ and }c_4 \}
\end{align*}
Just like in sinkless orientation, the edges become passive nodes. The first value of each triplet is the color of the nearest node, the second one is the color of the neighbor the edge is going towards and the third value is the color of the other neighbor.

\section{Round Elimination}\label{resection}

Brandt et al.~\cite{speedup} introduced round elimination in 2019. It is a procedure that takes a round elimination problem $\Pi_0$ and outputs a new problem $\Pi_1$. It has interpretations in various models of distributed computing. As the name suggests, $\Pi_1$ can generally be solved one round faster than $\Pi_0$.

While $\Pi_0$ assigns a color to each edge, $\Pi_1$ assigns a set of colors to each edge. That set contains the colors that the edge could have in a solution to $\Pi_0$. There is uncertainty because in one round less some information that affects the solution of $\Pi_0$ is out of reach.

\subsection{Lines}

Dennis Olivetti~\cite{RE} wrote an implementation of round elimination called Round Eliminator. Round elimination has been used to prove bounds on time complexity for various problems in the LOCAL model~\cite{tc1, tc2, tc3}.

In Round Eliminator, problems are represented as \emph{lines}. Lines are a kind of shorthand notation that compresses multiple configurations into one line. Each line is a multiset of sets and represents or \emph{generates} all the configurations obtained by choosing one color from each set.~\cite{RE}

For example, the line \reline{IO IO O} generates the configurations $[I, O, O]$, $[I, I, O]$ and $[O, O, O]$. ($[O, I, O]$ is the same as $[I, O, O]$, since lines and configurations are unordered.)

\begin{figure}[h]
  \centering
  \begin{tcolorbox}[width=.22\textwidth, nobeforeafter, title=active side]
  IO IO O
  \end{tcolorbox}
  \begin{tcolorbox}[width=.22\textwidth, nobeforeafter, title=passive side]
  I O
  \end{tcolorbox}
  \caption{Sinkless orientation in Round Eliminator's shorthand}
\end{figure}

\subsection{Formal definition}

Let $\Pi_n$'s alphabet, active side and passive side be $\Sigma_n$, $A_n$ and $P_{n}$ respectively. Let $\text{cfgs}$ be a function from a line to all the configurations it generates.

Note that the configurations of $\Pi_1$ have the exact same shape as lines of $\Pi_0$: multisets of nonempty subsets of $\Sigma_0$. This is why configurations of $\Pi_1$ generate configurations in the below definitition.

The definition of RE from Distributed Algorithms 2020~\cite{DA2020} rephrased in terms of lines:
\begin{itemize}
  \item $\Sigma_1$ is the powerset of $\Sigma_0$ minus the empty set.
  \item $A_{1} = \{x \mid \forall c : \text{cfgs}(x),\ c \in P_0 \}$
  \item $P_1 = \{x \mid \exists c : \text{cfgs}(x),\ c \in A_0 \}$
\end{itemize}

In words: $A_{1}$ consists of all lines that only generate configurations present in $P_{0}$. $P_1$ consists of all lines that generate at least one configuration from $A_0$.

\subsection{Interpretation in the port numbering model}

In this section I'll show that round elimination produces a problem that can be solved exactly one round faster than its predecessor in the port numbering model. The port numbering model is the weakest of a number of of models of distributed computing. But round elimination has been shown to work in stronger models, like the LOCAL model as well.~\cite{tc3}

\subsubsection{The port numbering model}

In the port numbering model, each vertex runs the same program and must produce part of a solution to a problem defined on its graph. The problem need not be locally checkable and the graph does not need to be a tree but this thesis is about round elimination problems, so everything documented here has that shape.

Computation proceeds in a series of rounds. In each round, vertices send a message to each of their neighbors. Then all vertices receive the messages simultaneously. Finally, each vertex can either output its part of the solution and stop, or continue to the next round. Before, after and between these steps, the nodes get to compute as much as they want. This process repeats until all vertices have chosen their output.

Depending on the problem, each node may also get some information when the execution starts. For instance the total number of nodes is often provided so each node can compute how long the algoritm is going to run in advance.

However, everything stated in the previous paragraphs applies equally well to the LOCAL model for example. The defining characteristic of the port numbering model is that the only thing each vertex knows is that it is connected to its $n$ neighbors via ports numbered $1$ to $n$.

Formally, we can define a graph in the port numbering model as a set of connections between ports.
\begin{align*}
  \text{Port} = \text{Vertex} \times \mathbb{Z^+} \\
  \text{Connection} = \{Port, Port\}
\end{align*}
Every port must be connected to only one other port. If a node's port $n$ is connected, ports $1$ to $n-1$ must be connected as well.

An algorithm in the port numbering model can be expressed as three functions.
\begin{align*}
  \text{send} &: \Pi_{n : \mathbb{N}}~\text{State} \to \text{Message}^n \\
  \text{recv} &: \Pi_{n : \mathbb{N}}~\text{Message}^n \to \text{State} \to \text{State} \\
  \text{result} &: \text{State} \to \text{Output} + 1
\end{align*}
The first one computes what messages to send based on the state the node is in; the second how a node's state changes when it receives its neighbors' messages. The first message in the tuple produced by $\text{send}$ is sent to port 1, the second to port 2 etc. Likewise, the first message in the tuple passed into $\text{recv}$ is the one received from port 1. Finally, the function called $\text{result}$ computes a node's output if it has stopped and returns unit otherwise. (The notation $\text{Output} + 1$ means $\text{Output}$ or unit. The one stands for a type that is inhabited by only one value.)

Note that $\text{send}$ and $\text{recv}$ may behave differently based on the number of ports. In fact, they have to because $\text{send}$ must produce a tuple of messages of the correct length. The number of ports, $n$ cannot be a normal parameter, as the type of the function depends on its value. For an in-depth discussion of the $\Pi$ notation see Section~\ref{pitypes}.

An $r$-round solvable problem is a problem for which there exists an algorithm that gives the correct output after $r$ rounds. It is easier to prove that round elimination works in the port numbering model by adopting a different perspective.

\begin{lemma}
  A problem on trees is $r$-round solvable in the port numbering model if and only if each vertex can produce a correct output by looking at its $r$-neighborhood.
\end{lemma}
Here $r$-neighborhood refers to the subgraph containing vertices and edges at most $r$ edges away.

\begin{proof}
  The $r$-neighborhood is sufficiently large, as information from further away cannot be seen in $r$ rounds.

  Every vertex can gather its $r$-neighborhood in $r$ rounds, so it isn't too much information either. The gathering process works as follows: Every round, every node sends through each of its ports the number of the port, its number of neighbors and the messages it received in the previous round along with any data it was initialized with.

  After round $r$, all vertices can reconstruct their $r$-neighborhood. The structure of nested messages exactly mirrors the structure of the graph.

  In general graphs, the neighborhood cannot be reconstructed because if two distinct paths lead to a node that looks the same, there is no way to tell if the paths go to the same node or not. But in trees there is only one path between any two vertices.
\end{proof}

In the LOCAL model this lemma holds for general graphs because the LOCAL model assigns a unique identifier to every node.

\subsubsection{Round elimination in the port numbering model}

Suppose there is an algorithm $G_0$ that solves the round elimination problem $\Pi_0$ in $r$ rounds, where $r > 0$. Let us construct an algorithm $G_1$ that operates on the same graph but the active nodes are passive and vice versa. Also suppose that $G_1$ finishes in $r-1$ rounds, so it makes a decision based on an $r-1$ neighborhood.

Suppose that $G_1$ is run on some vertex $v$. For each neighboring vertex $n$, $G_1$ computes all possible $r$-neighborhoods centered around $n$. $G_1$ then runs $G_0$ on each of the neighborhoods, noting the color $G_0$ assigns to the edge $\{v, n\}$. $G_1$ assigns the union of all those colors to $\{v, n\}$.

\begin{lemma}
  $G_1$ solves $\Pi_1$, the result of running round elimination on $\Pi_0$, provided that disjoint parts of the graph are independent.
\end{lemma}
\begin{proof}
  It suffices to show that $G_1$'s outputs satisfy $\Pi_1$'s active and passive side rule.

  All the edge colors surrounding a passive node are computed from the same part of the graph but with different blind spots. Since $G_0$ is run  with all possible values in the blind spots, there is a run where the blind spot has the same values as it does in reality. Thus each edge's set contains the color that $G_0$ would assign to that edge. Because $G_0$ solves $\Pi_0$, $A_0$ must contain a configuration with those colors. Therefore $P_1$ is satisfied, as all the passive nodes' surrounding edges generate a configuration from $A_0$.

  The edges $e_1, e_2, \ldots$ surrounding an active node are computed from neighborhoods whose blind spots are completely disjoint since the graph is a tree. Suppose the edges's colors generate some configuration $[a_1, a_2, \ldots]$ that is not in $P_0$. Then w.l.o.g.\ there exist some values for the blind spots such that $G_0$ colors $e_1$ with $a_1$, $e_2$ with $a_2$, etc. As the blind spots are completely independent, it is possible to set them all to those specific values at the same time, which results in a graph where $G_0$ colors the edges around a passive node with a set of colors that is not found in $P_0$ which is a contradiction, as we assumed that $G_0$ solves $\Pi_0$. Thus we know that the colorings of active nodes only generate configurations that are in $P_0$.
\end{proof}

Moving to the LOCAL model violates the independency requirement, as two blind spots cannot have the same unique identifiers. Another way to break independency is to choose a very large $r$. If $r$ is so large that it could contain the whole network, the size of the network could prevent the largest options for the blind spots from existing at the same time.

\begin{lemma}
  $\Pi_1$ can be solved at least one round faster than $\Pi_0$.
\end{lemma}
\begin{proof}
  Suppose that $G_0$ is the fastest possible algorithm that solves $\Pi_0$. $G_1$ solves $\Pi_1$ one round faster than that.
\end{proof}

\begin{lemma}
  $\Pi_1$ can be solved at most one round faster than $\Pi_0$.
\end{lemma}
\begin{proof}
  If there is an algorithm $G_1$ that solves $\Pi_1$ in less than $r-1$ rounds, it is possible to solve $\Pi_0$ in less than $r$ rounds: run $G_1$ on all passive nodes and send the resulting edge colors to the neighboring active nodes. The definition of $P_1$ guarantees that each active node is able to pick a configuration that is in $A_0$ from the edge colors around it. The definition of $A_1$ guarantees that the passive rule is satisfied no matter what color the active nodes pick.

  But we assumed that the fastest algorithm solves $\Pi_0$ in $r$ rounds, so the assumption that $\Pi_1$ can be solved in less than $r-1$ rounds must be false.
\end{proof}

I have shown that $\Pi_1$ can be solved at least one round faster and at most one round faster than $\Pi_0$. It follows that it can be solved exactly one round faster than $\Pi_0$.

\subsection{Optimizing output size}

Round elimination produces a gigantic number of configurations. Especially the new passive side contains almost every possible configuration. For example, suppose $\Sigma_{0} = {a, b, c}$ and $A_0$ has a configuration $[a, b]$. Now $P_{1}$ contains \reline{a~b} but also \reline{ab~b}, \reline{ac~b}, \reline{abc~b}, \reline{a~ab}, \reline{a~bc}, \reline{a~abc}, \reline{ab~ab}, \reline{ab~bc}, \reline{ab~abc}, \reline{ac~ab}, \reline{ac~bc}, \reline{ac~abc}, \reline{abc~ab}, \reline{abc~bc} and \reline{abc~abc}. If we were to use this directly, the problem size would grow at an alarming rate with successive applications of round elimination, so it makes sense to optimize output size over round elimination speed.

One simple optimization is to discard all active configurations containing symbols that don't appear in any passive configurations and vice versa, as using them is impossible. But even more can be discarded.

As mentioned earlier, the lines of $\Pi_0$ are just like the configurations of $\Pi_1$. (Which is why I am using line notation to write the latter as well.) In terms of lines, $A_1$ is the set of all \emph{valid} lines wrt. $P_0$, where valid lines are lines that only generate configurations that are in $P_0$.

I will now show that using only some valid lines as $A_1$ instead of all valid lines creates a problem has strictly less solutions than $\Pi_1$ but is exactly as hard.

\subsubsection{Domination}

Suppose the picture below is part of a solution to $\Pi_{1}$. One can see that \reline{x~a} and \reline{af~y} are in $A_{1}$ and \reline{a~af} is in $P_{1}$.

\begin{tikzpicture}[scale=2]
  \draw [thick, dashed] (0.3,0) -- (1,0) node[midway, above]{\{x\}};
  \draw [thick] (1,0) node[active]{n} -- (2, 0) node[midway, above]{\{a\}};
  \draw [thick, dashed] (3,0) -- (3.7,0) node[midway, above]{\{y\}};
  \draw [thick] (2,0) node[passive]{} -- (3, 0) node[active]{} node[midway, above]{\{a, f\}};
\end{tikzpicture}

If \reline{x~ac} is in $A_{1}$, there is also a solution where node $n$ chooses it instead of \reline{x~a}, provided that \reline{ac~af} is in $P_{1}$. \reline{a~af} is in $P_{1}$, so it generates some configuration from $A_{0}$. \reline{ac~af} generates strictly more configurations, so it is in $P_{1}$, too.

\begin{tikzpicture}[scale=2]
  \draw [thick, dashed] (0.3,0) -- (1,0) node[midway, above]{\{x\}};
  \draw [thick] (1,0) node[active]{n} -- (2, 0) node[midway, above]{\{a, \textbf{\underline{c}}\}};
  \draw [thick, dashed] (3,0) -- (3.7,0) node[midway, above]{\{y\}};
  \draw [thick] (2,0) node[passive]{} -- (3, 0) node[active]{} node[midway, above]{\{a, f\}};
\end{tikzpicture}

As every solution using \reline{x~a} can use \reline{x~ac} instead, \reline{x~a} can be omitted.

Any configuration $C$ in $A_1$ can be omitted if there is a $B$ in $A_1$ that can be paired up with $C$ so that for each pair $(c, b)$ where $c \in C$ and $b \in B$ it holds that $c \subseteq b$.~\cite{DA2020} For any $C$ and $B$ satisfying that criterion I say that $B$ \emph{dominates} $C$ and $C$ \emph{is dominated by} $B$.

\tikzset{
  EdgeStyle/.append style = {->}
}
\begin{figure}[h]
  \centering
  \begin{tikzpicture}
    \SetGraphUnit{2}
    \Vertex{a abc}
    \SO(a abc){a ac}
    \EA(a ac){a bc}
    \WE(a ac){a ab}
    \Edge(a abc)(a ac)
    \Edge(a abc)(a bc)
    \Edge(a abc)(a ab)
    \SO(a ac){a c}
    \EA(a c){a b}
    \WE(a c){a a}
    \Edge(a bc)(a b)
    \Edge(a bc)(a c)
    \Edge(a ac)(a c)
    \Edge(a ac)(a a)
    \Edge(a ab)(a a)
    \Edge(a ab)(a b)
  \end{tikzpicture}
  \caption{Graph of lines strictly dominated by \reline{a~abc}.}
\end{figure}

Note that domination is not the same relation as the subset relation of generated configurations. \reline{a abc ade} generates $[a, b, e]$, while \reline{a a bcde} only generates configurations that are in the former. Yet they are incomparable in terms of domination.

\subsubsection{Maximal form}

We don't need to include dominated lines in $A_1$, as a good algorithm solving $\Pi_1$ would never choose them, as the more dominant lines are always a better choice.

Thus the only lines needed in $A_1$ (the \emph{maximal form} of $P_0$) are the valid lines that no valid line strictly dominates. In other words, the \emph{maximal form} of $S$ is the Pareto front of \emph{valid} lines with respect to domination.

This work's main contribution is a new algorithm for finding the maximal form of $P_{0}$ by combining lines and a proof of its correctness.

\subsection{Aside: Line uniqueness}

Most sets of configurations can be represented by many different sets of lines but every line uniquely identifies a set of configurations. That is, there are no two lines that generate the same set of configurations.

I will show that there is only one way to turn configurations generated by a line back into a line.

Every color must be present in the line as many times as in the configuration that contains the highest number of it. Less obviously would not work and if it was present more times, there would be a configuration containing it that many times.

Using the information from the previous paragraph, it is possible to figure out the overlap between any two colors $a$ and $b$. If there is no configuration with all $a$s and all $b$s, one set in the line must have both. The magnitude of the overlap is the difference of how many $a$s and $b$s there are and the highest number of $a$ and $b$ seen at the same time.

The amount of overlap tells us how many sets there are which contain both $a$ and $b$. But some of those sets could also contain $c$. To figure out how many sets contain $a$, $b$ and $c$, we first figure out how many contains $a$ and $c$ and how many contain $b$ and $c$. That way we get a prediction for how many of all three are seen at the same time. The difference between the prediction and the observed value is how many sets contain all three.

Let $o$ be a function from sets of colors to the maximum number of them that is observed at once in a configuration and let $s$ be a function from sets of colors to the number of them or a superset present in the line.
\begin{align*}
  o(\{a\}) &= s(\{a\}) \\
  s(\{a, b\}) &= s(\{a\}) + s(\{b\}) - o(\{a, b\}) \\
  s(\{a, b, c\}) &= s(\{a, b\}) + s(\{a, c\}) + s(\{b, c\}) - o(\{a, b, c\}) \\
  \cdots
\end{align*}

Now the line can be reconstructed by repeatedly adding the largest set for which $s$ is nonzero and subtracting one from all values of $s$ associated with a subset of it. Any deviation from the process would yield a line which does not produce the same configurations, so the line is unique.

\subsection{Putting it all together}

To perform round elimination, one first finds the maximal form of $P_0$. The configurations of the new active side, $A_1$, are exactly that maximal form.

However, we want lines, not configurations, so the colors in each configuration get wrapped into a singleton set. For example, the $\Pi_1$-configuration $[\{a, b\}, \{c\}, \{a, c\}]$ turns into the $\Pi_1$-line $[\{\{a, b\}\}, \{\{c\}\}, \{\{a, c\}\}]$. Written using the notation for lines I use elsewhere it is \reline{\{a, b\} \{c\} \{a, c\}}.

The new alphabet, $\Sigma_1$ consists of all colors found in $A_1$.

The new passive side is built by replacing every set in the lines representing $A_0$ with all colors from $\Sigma_1$ that it intersects. For instance if $\Sigma_1 = \{\{b\}, \{c\}, \{a, b\}, \{a, b, c\}\}$, $\{a, b\}$ would be replaced with $\{\{b\}, \{a, b\}, \{a, b, c\}\}$.

Finally, to make the representation more compact, $\Sigma_1$ is converted from subsets of $\Sigma_0$ to ordinals simply by enumerating the colors.


\section{Maximization via line combination}\label{nlproof}

In this section I will present a maximization algorithm based on combining lines and prove its correctness. The lemmas closely follow ones I have proved in Coq. The details only relevant to the proof in Coq are discussed in Section~\ref{mechproof}.

\subsection{Combining lines}

The new maximization algorithm mostly consists of combining lines. Two lines can be \emph{combined} by pairing up their sets and taking the union of one pair and the intersection of the rest.

\begin{figure}[h]
\begin{align*}
  \{I,O\} &\cup \{O\} = \{I, O\} \\
  \{I,O\} &\cap \{I,O\} = \{I, O\} \\
  \{O\} &\cap \{I, O\} = \{O\}
\end{align*}
\caption{one possible way to combine \reline{IO IO O} with itself}
\end{figure}

\begin{theorem}[Combination is sound]
\label{sound}
A combination $C$ of lines $A$ and $B$ only generates configurations present in $A$ or $B$.
\end{theorem}

\begin{proof}
For each configuration $c$ generated by $C$, w.l.o.g.\ suppose the symbol chosen from the union is from $A$. The symbols that come from intersections are in both $A$ and $B$. Thus all symbols in $c$ are from $A$.
\end{proof}

\subsection{Domination relation}

A \emph{maximal line} is a line that no valid line strictly dominates. The \emph{maximal form} is simply a collection of all the maximal lines.

\begin{lemma}\label{domtrans}
The domination relation is transitive.
\end{lemma}

\begin{proof}
Let line $a$ dominate $b$ and $b$ dominate $c$. Choose some ordering for $b$. Order $a$ so that each of its sets includes $b$'s set. Order $c$ so that each of its sets is included in $b$'s set. As set inclusion is transitive, the corresponding sets of $a$ and $c$ are included in each other.
\end{proof}

\begin{lemma}\label{wfproof}
The strict domination relation is well-founded.
\end{lemma}

\emph{Well-founded} means that in any set, there is a minimal element. In our case that means that there is a line that doesn't strictly dominate any other line. This property is used later to perform induction on lines.

\begin{proof}
Define the weight of a line as the sum of its sets' cardinalities. Obviously no line can have a negative weight. If a line $a$ strictly dominates line $b$, then the weight of $b$ is less than the weight of $a$, since all of $b$'s sets are subsets of $a$'s and one is a strict subset.

A suitable minimal line for any set is the line with the lowest weight. If there was a line strictly dominated by the lowest-weight line, that line would have even lower weight, which is a contradiction.
\end{proof}

\subsection{Building lines}

A \emph{singleton line} is a line that generates just one configuration. It is the configuration with each symbol wrapped in a set.

\begin{lemma}[Line splitting]\label{linesplit}
For any non-singleton line $x$ there exist two lines strictly dominated by $x$ that can be combined into $x$.
\end{lemma}

\begin{proof}
Since $x$ is not a singleton line, one of its sets, $S = \{a, b, \dots\}$, must contain multiple elements. Make two lines: one where $S$ is replaced with $S \smallsetminus a$ and another where $S$ is replaced with $S \smallsetminus b$. $(S \smallsetminus a) \cup (S \smallsetminus b) = S$, so $x$ can be built out of them and $x$ strictly dominates them, as they have strictly less symbols in them.
\end{proof}

\begin{theorem}\label{explode}
Any line can be built by combining singleton lines that it dominates.
\end{theorem}

\begin{proof}
One can perform well-founded induction on lines because the strict domination relation is well-founded. Thus it suffices to show that if all lines strictly dominated by $x$ can be built from dominated singleton lines, $x$ can be, too.

If $x$ is not a singleton line, we can use the line splitting lemma (Lemma~\ref{linesplit}) to show that it can be built out of two lines that it strictly dominates. The induction hypothesis tells us that those lines in turn can be built from dominated singleton lines.
\end{proof}

\begin{lemma}[Bigger is better]\label{biggood}
If lines $a$ and $b$ can be combined into $c$ then $a'$ that dominates $a$ and $b'$ that dominates $b$ can be combined into $c'$ that dominates $c$.
\end{lemma}

It is clear that if $a'$ and $b'$ are combined in the same fashion as $a$ and $b$, the result cannot contain less symbols.

\begin{theorem}[Combination is complete]\label{complete}
A line dominating any valid line $x$ can be built by combining input lines.
\end{theorem}

\begin{proof}
According to Theorem~\ref{explode} $x$ can be built from singleton lines that it dominates. Those singleton lines are valid as well, since dominated lines generate strictly less configurations. Being valid singletons, they generate one configuration each, which is contained in some input line.

If one combines those input lines instead, the result is a line dominating $x$ according to Lemma~\ref{biggood}.
\end{proof}

\begin{corollary}
The maximal lines can be built by combining input lines.
\end{corollary}

\begin{proof}
A line is maximal if no valid line strictly dominates it. Thus a valid line dominating a maximal line must be equal to it. Lines obtained by combining input lines are valid; Theorem~\ref{sound} shows that combining does not enable new configurations.
\end{proof}

\subsection{Efficiently finding the maximal lines}

Even if combining lines eventually produces the maximal lines, it may not be an efficient way of finding them. Building a maximal line could require hundreds of combinations!

Define a \emph{missing line} as a valid line dominated by none of the input lines.

\begin{lemma}\label{biggermissing}
A line that dominates a missing line is also missing.
\end{lemma}

\begin{proof}
Let $a$ and $b$ be any lines such that $b$ dominates $a$ and $a$ is missing. Suppose that $b$ is not missing. Then there is an input line $i$ that dominates $b$. Because the domination relation is transitive (Lemma~\ref{domtrans}) $i$ dominates $a$ as well, so $a$ is not missing, which is a contradiction.
\end{proof}

\begin{theorem}\label{justtwo}
If there is a missing line, then some missing line can be obtained by combining two input lines.
\end{theorem}

In other words, we can make progress by simply trying all combinations of two lines.

\begin{proof}
According to Theorem~\ref{complete}, there is some way of combining input lines that produces the missing line. Since they are combinations of the input lines, all lines leading up to the missing line are valid due to Theorem~\ref{sound}.

By definition a line that is valid but not missing is dominated by some input line. Thus all of them are either missing or dominated by an input line.

Suppose that the lines that combine into the missing line are dominated by input lines. Combining those input lines in the right way yields a line that dominates the missing line according to Lemma~\ref{biggood}. Lemma~\ref{biggermissing} tells us that bigger line is missing as well.

If one (or both) of the lines are missing, recurse into the missing line. There will be a pair of non-missing lines eventually, as the combination starts with input lines.
\end{proof}

Theorem~\ref{justtwo} gives an efficient process for checking maximality: if all ways of combining two lines result in lines dominated by some existing line, then the current set of lines is maximal.

A trivial extension yields an algorithm for finding the maximal form: The input lines start in a todo set and there is an empty done set. As long as there are lines in the todo set, take one of them. If it is dominated by some other line in either set, discard it. Otherwise insert it into the done set and insert all combinations of it and lines in the done set into the todo set.


\section{Type theory as mathematical foundation}\label{dependent}

The Curry-Howard correspondence states that a program is a proof of a theorem corresponding to its type. Thanks to this idea, we can use a model of computation as the foundations of mathematics instead of the conventional Zermelo-Fraenkel set theory axioms.

\subsection{How to prove with code}

The notation $e : T$ is read as expression $e$ has type $T$. This is analogous to set membership in set theory if $T$ is seen as the set of all values that $e$ could have. Types are not sets and don't always behave in the same way but some operations look very similar for both. Notably, types don't support union and intersection like sets do.

Types that are checked at runtime, found for example in Python are not the same thing. In Python, the same expression can have a different type on different runs. I am only talking about statically typed languages, so every expression has just one type, which can be computed without running the program.

Constructing a value of type $T$ proves that something of type $T$ exists. However, in most programming languages it is possible to write a function that does not construct a value of type $T$ even though that is its return type. Apart from broken type systems, this happens because the languages allow programs to crash or loop. Section~\ref{totality} explains how Coq prevents that.

We can now prove that integers exist by writing $1: Integer$, which isn't very exciting. We can also encode propositional logic (see Listing~\ref{rustlog}) and integers. Some programming languages' type system is even Turing-complete but as far as I know, no type system is seriously used as a programming language. It would be much more useful to write theorems about the outputs of functions instead of some ugly type-level constructs. This is where dependent types come in.

\begin{listing}[h]
  \begin{minted}{Rust}
  let a_or_b: Result<A, B>;
  let a_and_b: (A, B);
  let not_a: &dyn Fn(A) -> Void;
  enum Void {}
  \end{minted}
  \caption{propositional logic in Rust types}
  \label{rustlog}
\end{listing}

A dependently typed programming language can express types that depend on values as opposed to simply typed languages, where types and values are entirely separate. For example, with dependent types, one is able to write a type $1 + 1 = 2$ where $1$, $2$ and $+$ are the same constructs that one would use in a program that performs calculations. Even quantifiers are implemented with dependent types in Coq.

Before going into dependent types it more detail, I'll briefly discuss the algebraic notation that is often used when discussing types.

\subsection{Type algebra}

The unit type $1$ is a type with only one inhabitant, commonly denoted as the empty tuple $()$. The type $0$ is a type that no value has. It should not be confused with \textbf{void} in C, which is actually $1$. If a function's return type is $0$, the function must never return, as it is impossible to construct a value of type $0$.

Values of a product type $A \times B$ consist of an $a : A$ followed by a $b : B$. The cartesian product is the corresponding set operation. Many programming languages have a tuple type, which is a pure product type. Record or struct types are common named variants of product types. In most cases, they could be replaced with tuples; the names are only for making them more readable.

Values of a sum type $A + B$ are either of type $A$ or of type $B$. The corresponding set operation is the disjoint union. A disjoint union is a union where the elements are augmented with an index indicating the set from which set they originate. In programming, this is known as tagged union. Enumerations are the simplest example; booleans could be written $1 + 1$, though it is more concise to call them $2$. A tagged union with zero variants cannot be constructed so it is $0$. Optionals found in Rust or Scala among others, are of the form $T + 1$. Structurally typed unions, an intermediate between C's wildly unsafe unions and tagged unions is found in gradually typed languages like TypeScript and in the joke language IntercalScript~\cite{ICS}.

Knowing this much type algebra is sufficient to understand why the dependent types presented next are called $\Pi$ and $\Sigma$-types but I'll go on to show that types equipped with the previously defined sum and product form a semiring just to highlight that there truly is an algebra of types.

\subsubsection{Aside: More type algebra}

I have many times found myself wondering if I should write a data structure $A \times (B + C)$ or $(A \times B) + (A \times C)$ when it isn't entirely clear if $A$ really represents the same thing in both cases. I am not aware of any programming language where such \emph{isomorphic types} can be used interchangeably. Incidentally, in Coq $((1, 2), 3)$ is equal to $(1, 2, 3)$ but $(1, (2, 3))$ has a different type.

\begin{theorem}
  The operations $+$ and $\times$ form a semiring on the isomorphism classes of types.
\end{theorem}

\begin{proof}
  It is clear that $+$ is associative and commutative. Its neutral element is $0$; a variant holding zero will never be constructed, so an added $0$ does nothing.

  We can translate between $((a, b), c)$ and $(a, (b, c))$, so $\times$ is associative. The neutral element of $\times$ is $1$, as it doesn't add any information, like struct padding in C.

  Distributivity: $A \times (B + C) \cong (A \times B) + (A \times C)$. As discussed earlier, $0$ is impossible to construct, so a tuple containing it is, too: $A \times 0 = 0$.
\end{proof}

Finding an isomorphism, a one-to-one mapping, is equivalent to showing that two sets have the same cardinality, so finite types can be identified by natural numbers. We already saw the types $0$, $1$ and $2$. For finite types, one could just have stated that $|A + B| = |A| + |B|$ and $|A \times B| = |A||B|$ and used the fact that natural numbers form a semiring. The problem with non-finite types is that depending on the type system, they may not correspond to ordinals or any other well-known kind of integers.

From the cardinality point of view, it is easy to see that the arrow $\to$ in function types corresponds to exponentiation; a function is just a table of outputs with an entry for each input.
\begin{equation*}
  |A \to B| = |B|^{|A|}
\end{equation*}
A table of things of type $T$ is just $T$ raised to some power.

Inductive types have interesting equations, for example
\begin{align*}
  \text{List} &= 1 + (A \times A) + (A \times A \times A) + \ldots
\end{align*}
is a solution to
\begin{align*}
  \text{List} &= (A \times \text{List}) + 1
\end{align*}
which is the typical definition of a list in functional programming languages. Conor McBride~\cite{typeDerivative} has even shown that the partial derivative of a type is a data structure representing an in-progress traversal of it.

\subsection{Examples of dependent types}
\subsubsection{Dependent functions}\label{pitypes}

A $\Pi$-type is a type that depends on the value passed in. For example, ordinarily it would be impossible to write a function that sums all entries in a tuple because tuples of different lengths have different types. Using a $\Pi$-type, the function could have the type $\Pi_{n : \mathbb N}~\mathbb N^n \to \mathbb N$.

In dependently typed programming languages, $\Pi$-types typically look like ordinary functions. For example, the previous example can be written in Coq as
\begin{minted}{Coq}
Fixpoint ntuple n T : Type :=
  match n with
  | 0 => T
  | S n' => ntuple n' T * T
  end.

Notation "T ^ n" := (ntuple n T).

Fixpoint sumt (n : nat) (nats : nat^n) :=
  match n return nat^n -> nat with
  | 0 => fun x => x
  | S n => fun a => let (r, x) := a in sumt n r + x
  end nats.
\end{minted}

The universal quantifier \icoq{forall x : T, P x} is another way to write a $\Pi$-type in Coq.

Why is it called a $\Pi$-type? A type $\Pi_{e:T}~D(e)$ could be seen as the image $D[T]$, which contains the value of $D$ for all values that $e$ can have. That image is a product type, which is denoted with $\Pi$ just like products on numbers.

\subsubsection{Dependent pairs}

If the naming of $\Sigma$-types follows the same pattern as the naming of $\Pi$-types, then $E = \Sigma_{e:T}~D(e)$ is the sum of the image $D[T]$. And it is! A value of type $E$ contains just one of the values that $D(e)$ can have. And we know the value of $e$, too, because $+$ is a disjoint union.

In dependently typed programming languages, $\Sigma$-types often look like ordinary data structures. One could add a data structure that stores a tuple of arbitrary length to the previous example:
\begin{minted}{Coq}
  Inductive tlist T := make_tlist (n : nat) (l : T^n).
\end{minted}
Explicit $\Sigma$-types are called dependent pairs by programmers. The Coq standard library defines syntax for dependent pairs that allows writing \icoq{tlist} as
\begin{minted}{Coq}
  Definition tlist2 T := { n & T^n }.
\end{minted}

Viewed as a proposition, $\Sigma_{x:T}~P(x)$ corresponds to $\exists x : P(x)$.

\subsubsection{Other exotic types}

$\Sigma$ and $\Pi$-types are not the only kinds of new types that can exist in dependent type theories.~\cite{hofmann1997syntax} For instance Rathjen~\cite{griffor1994strength} has shown that extending Martin-Löf type theory with well-founded tree types makes it a significantly stronger proof system.

Coq's \icoq{Inductive} definitions can be used to define many kinds of dependent types but not all of them; for example higher inductive types are not supported.

\subsection{Universes}

A universe hierarchy is a feature of many dependently typed languages that the user mostly doesn't encounter but that is required for consistency as a proof system. It prevents making a paradoxical type $P$ such that $P: P$. In a language with polymorphism, such a type can be used to prove anything via Girard's paradox.

In Coq, the type of a simple type is Set, the type of set is Type0, the type of Type0 is Type1 and the type of that is Type2 etc. Types have subtyping: any type may be used as if its universe index was greater.~\cite{CPDT}

Impredicativity tends to cause inconsistency in logical systems. Coquand~\cite{newParadox} reports on one such paradox he has discovered. The universe hierarchy forces predicativity, meaning that an object can't take itself as an argument. Such an object would have to have to live in a universe $U$ such that $U < U$.

Certified Programming with Dependent Types~\cite{CPDT} gives an example of a universe error (shown below) but it has to be slightly modified, as it compiles and runs without errors on Coq 8.15.
\begin{minted}{Coq}
Definition id {T: Type} (x: T) := x.
Check id id.
\end{minted}
The code typechecks because Coq's type inference has become good enough to figure out that it can choose \icoq{T := U -> U} in the first \icoq{id} and \icoq{T := U} in the second.

To get the error that the example tries to produce, we need to disallow inferring the implicit argument.
\begin{minted}{Coq}
Check id (@id).
\end{minted}
produces the expected error.
\begin{verbatim}
The term "@id" has type "forall T : Type, T -> T"
while it is expected to have type "?T"
(unable to find a well-typed instantiation for "?T":
cannot ensure that "Type@{id.u0+1}" is a subtype of "Type@{id.u0}").
\end{verbatim}

Especially with the improvements in inference, typical users rarely encounter universes. There is already experimental support for universe polymorphism~\cite{coqRefman} which completely eliminates the problem in the above example by allowing use of the same definition with different universe levels.

\subsection{Why Coq}

When I started this project, I chose Coq simply because it is very mature and I knew it to be powerful enough. I now know that type theory based proof assistants like Coq are especially suited to this kind of task because they are capable of \emph{reflection}.

Reflection means that one is allowed to prove something by writing a program that checks it, proving that program correct and then using the program in a proof. The key thing that enables reflection in Coq is that its language Gallina is both a programming language and a logic at the same time.

On the flip side, because of Coq's complexity, there are only relatively weak automatic theorem provers for it.

Certified Programming with Dependent Types~\cite{CPDT} has a chapter that goes through progressively more complicated uses of reflection.

To verify a program in a proof assistant that does not support programming, one has to generate lemmas from the source code that is being verified. The seL4 microkernel~\cite{sel4} has been developed in that fashion, for example. That makes sense because high performance and low level access to hardware is vital when implementing an operating system.

Besides, round elimination is commonly used as a subroutine in proofs, while operating systems are not. Embedding round elimination in Coq makes it possible to prove whole reasoning chains, while a verified implementation of only round elimination could be given the wrong input.

\section{Mechanized proof}\label{mechproof}

I have formalized the proof presented in Section~\ref{nlproof}, written an implementation of maximality checking in Coq and proved that implementation's correctness. The proofs involve many boring details, which this thesis does not cover. However, I will show examples of the most important techniques used.

\begin{listing}[h]
\begin{minted}{Coq}
Definition cannot_find_more :=
  [forall a in input, [forall b in input,
    [forall c in all_combinations a b, ~~ (missing c && nonzero c)]]].

Definition none_dominated :=
  [forall a in input, forall b in input, ~~ (a ⊂ b)].

Definition is_maximal_form :=
  cannot_find_more && none_dominated.
\end{minted}
\caption{The definition of the verified maximality checking function.}
\label{is_maximal_form}
\end{listing}

This section is written for a reader that has some familiarity with statically typed functional programming but hasn't used Coq. I would encourage readers that have already written some mechanized proofs of their own to study the proof's source code~\cite{source_code} along with the natural language proof (Section~\ref{nlproof}) that closely follows it.

\begin{listing}[h]
\begin{minted}{Coq}
Theorem is_maximal_form_works :
  is_maximal_form <->
  (∀ x : line, nonzero x -> [exists y in input, perm_eq x y] <-> maximal x).
\end{minted}
\caption{The theorem that states that \icoq{is_maximal_form} returns true only iff the input consists of the maximal lines. That is, every line in the input is maximal and every maximal line is in the input.}
\end{listing}

\subsection{How to read Coq}

Coq very closely resembles OCaml, the language it is implemented in. The main syntactic difference is that top-level definitions start with a capital keyword and end with a full stop.

The majority of those statements are proofs, which may be started with \icoq{Lemma} or \icoq{Theorem}. They can be used interchangeably. I have used \icoq{Theorem} to mark the most important ones, which are also called theorems in the natural language proof (Section~\ref{nlproof}). Between the keyword and the first stop is what is to be proved. The the proof follows, terminated by \icoq{Qed} or \icoq{Defined}. Proofs terminated by the latter are allowed to be executed as programs.

Printed on paper, the proofs are typically completely incomprehensible. The reason is that they manipulate the state of the proof but that state is invisible in the source code. This is the complete opposite of paper proofs, which typically report the state after every transformation rather than how they got there.

\begin{listing}[h]
\begin{minted}{Coq}
Lemma permutation_contains : ∀ a (s s' : line), perm_eq s s' -> a ∈ s = a ∈ s'.
  move=> c.
  apply: perm_rew_helper => s s' pe.
  rewrite /contains/contains_unpermuted.
  move=> /existsP[cs /andP[pe_cs q]].
  move: pe; rewrite perm_sym; move=> /tuple_permP [mapping smap].
  fill_ex [tuple tnth cs (mapping i) | i < Δ].
  apply/andP; split.
    apply: perm_trans; last by apply: pe_cs.
    by apply: any_perm.
  move: q => /all2_tnthP q.
  rewrite smap; apply/all2_tnthP; move=> i.
  rewrite !tnth_map.
  by apply: q.
Qed.
\end{minted}
\caption{A sample proof. It should be clear what it proves but the proof itself is unreadable.}
\end{listing}

To allow curious readers to see everything without having to install anything, I have generated a web page \footnote{\url{https://joonazan.github.io/line-combination-proofs/}} with Alectryon~\cite{alectryon}, which allows browsing the intermediate steps.

Proofs look different from the rest of the code because Coq is a combination of the pure functional programming language Gallina and Ltac, an untyped logic programming language used to generate Gallina. I will not show any more proofs due to the readability issue. However, Section~\ref{ltac} explains how Ltac works.

\subsection{Notations}

Coq source code can look more exotic than OCaml because it is very easy to extend the syntax of Gallina.

My code uses the syntax defined in Mathematical components, for instance \icoq{n.-tuple T} is a length $n$ array with elements of type $T$ and \icoq{'I_n} is the type of integers from zero to $n-1$.

I define some binary operators for convenience: $\subseteq$ to denote domination, $\subset$ for strict domination and $\in$ to denote that a line contains a coloring.
\begin{minted}{Coq}
  Notation "a ⊆ b" := (dominates b a) (at level 50, no associativity).
\end{minted}

Notations are not limited to binary operations or fixed size syntax. For example the list literal \icoq{[a; b; c]} is defined as a notation in the standard library.

\subsection{Variables}

\emph{Variables} like
\begin{minted}{Coq}
  Variable alphabet_size: nat.
\end{minted}
can be used in function definitions within the current section. If a function that uses them, even indirectly, is called from another section, the variables need to be given as arguments.

For example, my definition of the type \icoq{color}
\begin{minted}{Coq}
  Definition color := 'I_alphabet_size.
\end{minted}
uses \icoq{alphabet_size}. My definition of configurations
\begin{minted}{Coq}
  Definition configuration := Δ.-tuple color.
\end{minted}
uses \icoq{color}. Now configuration takes \icoq{alphabet_size} (and \icoq{delta_minus_one}) as arguments, even though the definition has no parameters.

Variables are very convenient; without them I would have to explicitly make almost every lemma depend on the alphabet size and graph degree.

I use the variable \icoq{delta_minus_one} instead of $\Delta$ to ensure that $\Delta$ is positive. That way I do not need to deal with the uninteresting case where vertices are not connected at all. I could instead add \icoq{Variable nonzero_delta := Δ <> 0} but then I would have to explicitly use it every time to eliminate the zero case.

Encoding a natural number that is at least $k$ as $n + k$ composes very well. For example calling a function that needs something of size $m + 1$ with something of size $n + 2$ works. Both Coq's default nats and ssreflect's nats are Peano numbers, so in the aforementioned scenario the type inference engine has to unify \icoq{S m} with \icoq{S (S n)}, which results in a substitution of \icoq{m} by \icoq{S n}.

\subsection{Representation of lines}

\begin{minted}{Coq}
Definition nline n := n.-tuple {set color}.
Notation "n .-line" := (nline n) (at level 30, no associativity).
Definition line := Δ.-line.
\end{minted}

I chose to represent lines and configurations as tuples, so their lack of order is simulated by operating on some permutation instead of the actual line. This choice of representation isn't ideal as discussed in Section~\ref{badds}.

Both containing a configuration and beign a combination of two lines is defined as an auxiliary procedure operating on two ordered lines and a predicate that holds if there is some permutation of the lines that satisfies the auxiliary procedure.

\begin{minted}{Coq}
Definition contains_unpermuted (cl: configuration) (l: line) : bool :=
  all2 (λ c (s : {set color}), c \in s) cl l.

Definition contains (cl : configuration) (line : line) : bool :=
  [exists (p : configuration | perm_eq p cl), contains_unpermuted p line].

Notation "a ∈ L" := (contains a L) (at level 50, no associativity).
\end{minted}

The choice of permuting the configuration in \icoq{contains} is arbitrary. Proving \icoq{perm_eq s s' -> a ∈ s = a ∈ s'} would be a lot easier if the line was permuted, so it could be argued that a symmetric definition where a permutation of the configuration is compared against a permutation of the line would be best.

\begin{minted}{Coq}
Definition combine m (n := m.+1) (a : n.-line) (b : n.-line) : n.-line :=
  [tuple of (thead a) :|: (thead b)
  :: map (λ ab, ab.1 :&: ab.2) (zip (behead a) (behead b))
  ].

Definition combination_of (a b c : line) :=
  ∃ (a' b' c' : line),
    perm_eq a' a ∧ perm_eq b' b ∧ perm_eq c' c ∧ combine a' b' = c'.
\end{minted}

Because I never perform case analysis on \icoq{combination_of}, it is defined as a proposition, whereas contains is defined as a decision procedure.

\subsubsection{Zero lines}

The paper proof never deals with lines that contain an empty set because those aren't lines. The line data structure defined here allows those invalid lines with zero configurations. Most proofs hold for them as well some of the proofs leading up to the conclusion only accept proper lines by requiring them to satisfy
\begin{minted}{Coq}
Definition nonzero := all (λ x : {set color}, #|x| != 0).
\end{minted}

\subsection{Inductive definitions}

An \icoq{Inductive} definition in Coq is very similar to what is known as a generalized algebraic data type (GADT) in Haskell.

Type constructors in general can be thought of as opaque functions that can be performed in reverse. GADTs make this explicit, as they are defined by writing the type of each type constructor. Like most things in Coq, the constructors in inductive definitions can be dependently typed.

Depicted below is the only inductive definition in my code. They usually define data structure but mathematical components already contains the data structures that I need, so this one is used as a specification. It is a proof that a line can be obtained by repeatedly combining input lines. There are two cases: either the line is in the input, or the line can be constructed from two other lines that can be constructed out of input lines.

\begin{minted}{Coq}
Inductive iterated_combination : line -> Prop :=
    present : ∀ a, a \in input -> iterated_combination a
  | combined : ∀ (a b c : line), iterated_combination a -> iterated_combination b ->
      combination_of a b c -> nonzero c -> iterated_combination c.
\end{minted}

Data structures like this are just like Prolog specifications. The same could be written in Prolog as follows.
\begin{minted}{Prolog}
iterated_combination(A) :- in_input(A).
iterated_combination(C) :-
  combination_of(A, B, C),
  iterated_combination(A),
  iterated_combination(B).
\end{minted}
But Prolog has a fixed evaluation strategy. In Coq, the evaluation strategy is controlled by writing tactics (see Section~\ref{ltac}).

These kinds of specifications are generally very easy to read but in this one it is not clear why \icoq{c} needs to be \icoq{nonzero}. But that doesn't matter because this one is only used in an intermediate step, rather than the final theorem's specification.

\subsection{Totality}\label{totality}

Every function in Gallina is \emph{total}, which means that all programs eventually complete without crashing if executed on correct hardware with a sufficient amount of memory.

Totality is necessary because it would be easy to prove anything with a nonterminating function, as the return type of a function that never returns can be anything.
\begin{minted}{Coq}
Fixpoint make_false n : False := make_false (n+1).
\end{minted}
Fortunately attempting to define the above function produces the error `Recursive definition of make\_false is ill-formed.` Coq requires that recursive functions have at least one \emph{decreasing argument}.~\cite{coqRefman} In recursive calls, the decreasing argument must be \emph{structurally} smaller; It must be only part of what was passed in. For example, a function that takes a list could call itself with just the tail of the list.

It can be shown that Gallina is not Turing-complete because all Gallina programs eventually halt. It is well known that checking if an arbitrary program halts is undecidable. But the halting problem is trivial for all Gallina programs, so Gallina can express only a subset of all programs.

In practice the limitation is rarely detrimental. Totality prevents writing a function that produces the Collatz sequence starting from an arbitrary integer because nobody has been able to prove that that sequence is always finite. One can still compute the first million numbers, which is the same thing for all practical purposes.

Deliberate infinite loops are a more common occurrence than problems that push the limits of mathematics. For example a web server is usually supposed to serve until it is stopped. However, serving one request must complete, so it suffices to wrap a Coq program in an infinite loop. In general, one can write a Coq program that computes a new state based on an event and feed it one event and the current state in an infinite loop.

Like recursive functions, inductive data structures are also limited to avoid infinite loops. They have to satisfy the positivity condition, which I will not explain in detail, as knowing it isn't necessary to write this the proof discussed in this thesis. However, I will give an example of a data structure that would lead to nontermination if allowed.

Suppose there was a data structure
\begin{minted}{Coq}
Inductive Wrapper T :=
  Wrap : (Wrapper T -> T) -> Wrapper T.
\end{minted}
that wraps a function that takes a \icoq{Wrapper T} and returns something of type \icoq{T}. (That data structure is disallowed by Coq's positivity check.) Now we can write a completely safe recursion-free function
\begin{minted}{Coq}
Definition unwrap {T} (w : Wrapper T) : T :=
  match w with Wrap f => f w end.
\end{minted}
that takes a Wrapper and calls the function inside it with itself.

The previous function is actually just a helper for
\begin{minted}{Coq}
Definition materialize T : T :=
  unwrap (Wrap unwrap).
\end{minted}
which returns something of type \icoq{T} for any \icoq{T}. Which is very bad because it can create a proof for any statement, even false statements. It does that by calling \icoq{unwrap} with \icoq{Wrap unwrap} in an infinite loop.

\subsubsection{Non-structural induction}

Not all total functions have a decreasing argument. My proof of
\begin{minted}{Coq}
Theorem combination_is_complete :
  ∀ line, valid line -> nonzero line -> ∃ line',
    iterated_combination line' ∧ line ⊆ line'.
\end{minted}
is a recursive argument that repeatedly applies to lines strictly dominated by the current line until eventually reaching a singleton line (see Section~\ref{complete}). A strictly dominated line is not structurally smaller, but it is still relatively easy to see that a chain of strictly dominated lines cannot go on forever. In other words, it is a well-founded relation.

Coq's standard library contains proofs for utilizing well-foundedness. They don't extend Coq's totality checking, they convert well-founded induction to structural induction.

I use a weight function to map lines to natural numbers just like in Proof~\ref{wfproof}.
\begin{minted}{Coq}
Definition weight (l : seq {set color}) := \sum_(i <- l) #|i|.
\end{minted}
It is easy to prove \icoq{well_founded strictly_dominated} using the helper lemma
\begin{minted}{Coq}
Lemma strictly_dominated_wf' : ∀ len (a : line),
  weight a < len -> Acc strictly_dominated a.
\end{minted}
since \icoq{well_founded} is defined as \icoq{forall a, Acc R a}, meaning that every inhabitant of a type is accessible via the relation R. Accessibility is enforced by the \icoq{Acc} type's sole constructor
\begin{minted}{Coq}
  Acc_intro : (forall y:A, R y x -> Acc y) -> Acc x.
\end{minted}
which requires that all smaller elements are also accessible. (The minimal elements are accessible because there are no smaller elements with respect to them.)

The helper lemma is proved by performing induction on weight and applying the lemma
\begin{minted}{Coq}
  Lemma strictly_dominates_weight (a b : line) : a ⊂ b -> weight a < weight b.
\end{minted}
relating weight to strict domination. With well-foundedness proved, we can use \icoq{well_founded_induction} from the standard library in \icoq{combination_is_complete}.

\subsection{Ltac}\label{ltac}

Ltac is the tactics language of Coq. It is mostly used to generate proofs. Sometimes it is used to derive a function whose signature forces the definition to be correct, for example decidable equality, a function that returns a proof that its arguments are equal or a proof that they are not equal. If a function is tricky to define because it requires proofs it is sometimes defined using the \icoq{refine}-tactic, which takes Gallina code with blanks and allows filling those blanks using Ltac.

Ltac isn't meant for generating normal code or specifications; its library focuses on generating any code that fits a type and it is less trustworthy than the core of Coq.

Code generation is a game of filling in blanks. At the start, there is one blank for the whole function body. It is replaced with some code that in turn contains one or more blanks. When all the blanks have been filled, the function is complete.

The blanks are just like typed holes in Haskell; they show the user the expected type and the typing environment i.e. the variables that are accessible from the hole. Coq calls the type of the hole the goal and the environment the set of hypotheses. Figure~\ref{proofview} shows how they look to a user.

Note that the hypotheses only show the local environment. Things defined outside the current function are also in the environment but they are not shown, as there can be thousands of them. There are commands like \icoq{Search} for quickly finding a helpful lemma out of the full environment.

\begin{figure}[h]
  \includegraphics[scale=0.7]{proof_view}
  \caption{The proof view. At the top are the hypotheses, below the solid line is the goal and below the dashed line is a currently unfocused goal.}
  \label{proofview}
\end{figure}

The basic building blocks of Ltac are tactics written as plugins in OCaml. One of these tactics can be invoked by writing its name and possibly some arguments followed by a stop. For example, \icoq{split.} breaks goals like \icoq{A /\ B} into multiple goals.

Multiple tactics can be chained together with a semicolon. If a tactic results in multiple new goals, the second tactic is applied to each one of them. For example, if \icoq{A} and \icoq{B} from the previous paragraph are easy to prove, \icoq{split; auto.} could completely prove it.

Useful combinations of tactics can be given their own name. For example, I have written
\begin{minted}{Coq}
  Ltac split_and := repeat (split || (apply/andP; split); try done).
\end{minted}
which splits everything it can and tries to convert boolean "and" to propositional "and" if it can't split and automatically proves all trivial goals using the \icoq{done} tactic from ssreflect.

Ltac can even select a tactic by pattern matching on the goal and hypotheses. I don't use it in this development but it can be very useful, especially when automating large parts of a development. A significant limitation of the technique is that the match is purely syntactic; unlike equality in Coq, pattern matching in Ltac does not consider two terms that evalute to the same normal form equal.

Mathematical components defines a lot of tactics and tactic notations (same as a notation but for Ltac) that I use, so my proofs don't look like standard Coq. The reason for the abundance of notations is that the library offers highly compact notation for writing precise tactics rather than fixing the lack of precision with powerful automation.

\subsection{Mathematical components}

Dealing with permutations seemed like something that could benefit from existing theories, so I switched to the Mathematical components (math-comp) library at that point in the development. It was originally created as part of a formal proof of the Four color theorem. It contains theorems about abstract algebra and common data structures.

Mathematical components is built on the small-scale reflection (ssreflect) library, which has a strong opinion on how Coq proofs should be written and provides utilities for writing in that particular style. I have adopted this style in my proof.

\subsubsection{Reflection views}

An important idea in ssreflect is that every proposition is written as a boolean expression and converted into the most suitable form via a reflection view when necessary.~\cite{mathcompbook}

The boolean form is excellent for case analysis and proofs via computation. Case analysis on propositional forms isn't possible without adding the law of excluded middle $\forall a : a \lor \lnot a$ as an axiom. It is common to avoid axioms if possible in Coq because not all axioms are compatible with each other.

Suppose we would like to separately prove the case where $a$ is equal to $b$ and the case where they differ. Via small-scale reflection we can view their equality as a boolean and perform case analysis on it. In conventional Coq, one would have to write a function for the type of a and b that either returns that its arguments are equal or that they are not. For equality, the standard library defines a tactic \icoq{decide equality} that fills in the body of such a function but that only works for equality. Ssreflect provides reflection conversions between boolean and propositional forms of equality, implication, negation and quantifiers among others.

The propositional forms are useful because unlike the boolean forms, they are data structures that can be operated on. An assumption that is a conjunction can be broken down into two assumptions. Case analysis can be performed on a disjunction. Propositional equalities can be used to rewrite.

\subsubsection{The ssreflect proof language}

Ssreflect also focuses on maintainability of proofs. A small change somewhere could invalidate a proof. Because many tactics never produce an error, code that previously completed a subgoal may leave that goal incomplete, causing weird errors in the tactics that were meant to be applied to the next goal. To prevent this, ssreflect advocates the \icoq{by} tactical, which produces an error if the tactic following it does not discharge a goal.

Another issue that ssreflect attempts to remedy is that most of the standard tactics in Coq create assumptions with automatically generated names. The names can be confusing and the only way to find out where some assumption \icoq{H1} is coming from is to step backwards in the proof until it dissapears. Because names are assigned sequentially, changing the beginning of a proof can change the name of every assumption, leading to a lot of tedious renaming.

Ssreflect's solution to naming is to add new assumptions to the goal so they do not immediately need a name. After adding an assumption \icoq{X} to a goal \icoq{G}, the new goal is \icoq{X -> G}. There are many bookkeeping tactics to rearrange the chain of implications like the stack in a language like FORTH.\@To allow the user to completely avoid automatic naming, ssreflect contains its own set of tactics that replaces almost all standard tactics.

When shuffling assumptions around isn't enough, they can be removed from the goal and named with the \icoq{=>} tactical, which can be appended to any tactic. For example, to name two assumptions produced by the tactic \icoq{tac}, one could write \icoq{tac => a b.} There is in fact a pretty rich language that one can use after the arrow. Most importantly, breaking products into their parts is done with a pair of square brackets and applying a function or reflection view is done with a slash. For example \icoq{/andP[a b]} takes the first assumption in the stack, turns it from a boolean and to a propositional and, splits it into two assumptions and names them \icoq{a} and \icoq{b}.

Instead of manual naming, Chlipala~\cite{CPDT} prefers to write very short, almost completely automated proofs. They are maintainable because they do not refer to named assumptions, instead defining automated procedures for processing certain shapes of goals or assumptions. My attempts at that level of automation haven't been very successful but I can see that my proof could be significantly shorter if some of the more trivial work was automated. Proofs with both heavy automation and cleaner presentation in the style of ssreflect could even be somewhat readable.

A significant downside of Mathematical components is that it defines its own version of everything. Even the \icoq{nat} in ssreflect is incompatible with the standard library's \icoq{nat}. Because of this standard \icoq{lia} tactic, which automatically solves linear integer arithmetic (equations involving comparison, addition and multiplication with constants) can only be used after converting every expression to the equivalent on standard nats. I solved this problem by writing a tactic called liafy that converts a subset of mathematical operations.

\begin{minted}{Coq}
  Ltac liafy := rewrite -?(rwP leP) -?(rwP ltP) -?(rwP negP) -?(rwP eqP) -?plusE.
  Ltac sslia := liafy; lia.
\end{minted}

Thanks to Mathematical components I didn't have to write any data structures myself, which makes me think that it is well suited for people who know abstract algebra very well but know little about programming. Writing the basic operations on a data structure in Coq can be very unintuitive especially because the convoy pattern~\cite{CPDT} is often necessary.

For example, when pattern matching on an array's length, the code in the match arm corresponding to zero still doesn't know that the array is empty. It seems that there is no way around it other than proof mode but actually one can very tediously annotate the pattern match, which makes it work in the desired way.

Pattern matching in many other languages is much easier to use but they typically rely on axiom K for pattern matching, limiting the settings the language can be used in; for example, Homotopy type theory's Univalence axiom is incompatible with axiom K. Cockx et al.~\cite{withoutK} developed and formally proved a version of dependent pattern matching without K in 2014 that is available in Agda today.

\section{Performance}\label{performance}

Maximization is the most expensive part of the original Round Eliminator's algorithm. Maximization via line combination is orders of magnitude faster in practice and is resistant to some pathological inputs but it remains the most computationally expensive part of round elimination. % TODO add to abstract maybe?

Evaluating the performance of maximization is difficult, as its output can be exponentially larger than its input. However, there is no known algorithm for checking maximality that is more efficient than performing maximization. Therefore I will compare different algorithms' performance on the decision problem of finding out whether a set of lines is maximal or not.

\subsection{Time complexity of the best previously known algorithm}

The maximization algorithm previously used in Round Eliminator relies on a procedure that transforms a maximal form into a maximal form with one configuration removed. If every configuration was valid, the maximal form would be a single line that generates every configuration, so that line is the starting point. Then each invalid configuration is removed one by one.

The algorithm sees all permutations of a line as separate lines and deduplicates only at the end. I will refer to them as lines and configurations in this description even though I mean their ordered counterparts.

To remove an invalid configuration from a line, the line is split into $\Delta$ different versions that cannot produce the invalid configuration because one of the symbols in the configuration has been removed from each.

The algorithm computes the set of invalid configurations by filtering all $|\Sigma|^{\Delta}$ possible configurations. It produces $\Delta!$ permutations of the output lines before the removal of permutations.

After removing each configuration, dominated lines are discarded with an $O(\Delta n^2)$ pass.

It is unclear how exactly the size of the maximal form develops, so the average number of lines is unknown. Assuming that the number of lines grows linearly, the time complexity of checking maximality is $O(|\Sigma|^{\Delta}{(n\Delta!)}^2)$.

There is an adversarial input that defeats this algorithm. Just concatenate multiple copies of an input, renamed so that each has its own disjoint subset of the alphabet. Then the number of valid lines grows linearly with the size of the input but the number of possible configurations grows to the power of the degree.

\subsection{Time complexity of line combination}

To check maximality with the new algorithm, one has to combine every pair of lines in every possible way and compare the results to the original lines.

The lines can be combined in $n^2\Delta\Delta!$ different ways, $n^2$ ways to choose lines, $\Delta!$ ways to match them up and $\Delta$ ways to choose which pair to take the union of. However, most of those ways result in lines that can be discarded because they contain an empty set or because they are dominated by the original lines or some other combination.

Computing the combinations is feasible even for large $\Delta$ when the lines contain many duplicated sets because in that case the order of the duplicated elements has no effect on the outcome. This seems to happen often in practice. However, it is an open question if it is possible to design an algoritm that does not have a factorial worst case.

Comparing two lines can be done by building a bipartite graph where each bipartition represents one of the lines and connecting one side's sets to every set on the other side that is included in them. If there is a perfect matching, one line dominates the other. Bipartite perfect matching can be solved in $O(\Delta^{2.5})$ via the Hopcroft-Karp algorithm. In practice it seems that a backtracking search can be faster but that could be asymptotically worse.

The resulting time complexity is $O(n^2\Delta\Delta!n\Delta^{2.5}) = O(n^3\Delta!)$. Notably, the size of the alphabet has no effect on running time anymore.

In practical implementations there are tricks for quickly discarding most results of line combination, so the power of $n$ is closer to 2 than 3. But I haven't proved that.

\subsection{In practice}

I and Dennis Olivetti both implemented the new algorithm in the Rust programming language. Both are publicly available at https://github.com/joonazan/maximizer and https://github.com/olidennis/round-eliminator respectively.

Both implementations compare new lines against other lines originating from the same pair of lines before comparing against the rest. This is worthwhile based on the assumption that many of the new lines are just inferior versions of the best lines of that batch.

Olivetti showed me that maintaining a set of dominated lines and checking new lines against that set before doing any comparisons improves performance by an order of magnitude in practice, which means that at least in the inputs tested, tens of copies of some useless lines are created.

I would have liked to measure the performance of the Coq function \icoq{is_maximal_form} but it turns out that even on the smallest possible input it requires a finite but large amount of memory. The next section explains how that shortcoming could be rectified.


\section{Discussion}

My goal was to build a provably correct version of round elimination while also improving its performance. I chose to focus solely on maximization, as it is the only nontrivial part of round elimination.

The line combination algorithm for maximization delivers on the performance goal and is already in use but the machine-verified procedure for checking maximality needs more work to be of practical use. This section outlines how one could make the maximality checking program practical and how the proof could be extended to cover round elimination or even whole proofs of time complexity upper or lower bounds.

\subsection{Impractical data structures}\label{badds}

The formalization of round elimination uses many data structures and proofs provided by the Mathematical Components library. However, that library is made with pure mathematics in mind; functions using the data structures aren't meant to be executed.

One example is the definition of \icoq{n.-tuple}, a fixed size list. It is simply a wrapper around a list and a proof that the list's length is \icoq{n}. As the tuple goes through various transformations, the proof grows longer, as it is always replaced with a new proof that references the old one.

I have observed that the proof's memory use grows much faster than linearly with the length of the tuple. I suspect that this is due to some proof that iterates through all permutations of the tuple. In any case, it is clear that actually executing the proof code is not viable.

The most common solution to this kind of problem is proof irrelevance. The idea is that theorems can be types that never have more than one inhabitant. That forces proofs be compile-time only: since the proofs have only one possible value, runtime behaviour cannot change based on that value. Coq has the type Prop, which can be used to mark proofs as irrelevant at runtime.

But Mathematical Components does not use Prop. It is not clear to me why that is hard.

It is possible to simply replace the data structures with efficient ones, which, given the size of the proof, could be done in reasonable time. Or one could refine (see Section \ref{transport}) the functions using CoqEAL to operate on more efficient data structures without changing the proof.

\subsection{A verified Round Eliminator}

Distributed algorithms researchers can find lower bounds for the time complexity of problems by making the problem easier by allowing strictly more configurations, performing round elimination on the result, then making it easier again, etc.\ until they arrive at some well-known problem. It can then be argued that the first problem must be at least as hard as the known problem. Similarly, upper bounds can be proved by making problems harder.

The Round Eliminator software has tools that can be used to quickly perform a series of simplification and round elimination steps. Each one of the tools produces some new problem that has some relationship to the current problem: easier, harder, exactly as hard or exactly one round easier.

\begin{figure}[h]
  \centering
  \includegraphics[scale=0.5]{retools}
  \caption{The tools menu of Round Eliminator}
\end{figure}

Each of the tools could be written as a function in Coq, along with proofs of the relationship of their input and output. With a bit of plumbing, these could be used to write reflective proofs like the following:

\begin{minted}{Coq}
Lemma a_harder_than_b : time(A) >= time(B).
  add_line [[C, D], [D, E, F]] A.
  speedup.
  eauto using time_gt_trans.
Qed.
\end{minted}

These could even be automatically generated by Round Eliminator. In cases where Round Eliminator has performed an expensive brute force search, the result of the search can be embedded into the Coq proof in order to avoid running the slower Coq implementations of the tools a lot.

As part of this thesis I have only verified the difficult part of round elimination, maximization. Another possibly difficult part which remains unexplored is formalizing the time complexity of distributed problems. Given a formalization of time complexity, verifying the other tools in Round Eliminator is easy, as they do very simple transformations: adding a line makes the problem easier, removing one makes it harder, renaming a symbol to another makes the problem easier, etc.

Choosing an operation that helps prove a lower bound is nontrivial but that procedure does not need to be proved correct. Were a good series of operations found, it would just need to be checked with the verified versions of the Round Eliminator tools.

\clearpage
\thesisbibliography{}

\bibliographystyle{plainnat}
\bibliography{correct_round_elimination}

\end{document}
